## Recent Advances in Physics-Informed Large Language Models (LLMs)

### Introduction
Physics-Informed Large Language Models (LLMs) represent an innovative integration of machine learning with physical sciences. These models leverage the structure and principles of physical laws to enhance their predictive accuracy and efficiency. Here, we explore some of the most recent advances in this rapidly evolving field.

### Recent Publications
- Lin, F., Liu, J., Li, X., Zhao, S., Zhao, B., Ma, H. and Zhang, X., 2024. PE-GPT: A Physics-Informed Interactive Large Language Model for Power Converter Modulation Design. arXiv preprint arXiv:2403.14059. [Link](https://arxiv.org/abs/2403.14059)
  ![image](https://github.com/qiaosun22/AwesomePhysicsInformedLLMs/assets/136222260/3b649ee5-1c36-413e-b8ad-11b75fb44693)
  The paper proposes PE-GPT, a custom-tailored large language model uniquely adapted for power converter modulation design. By harnessing in-context learning and specialized tiered physics-informed neural networks, PE-GPT guides users through text-based dialogues, recommending actionable modulation parameters.


### Literature Reviews

#### Neural Symbolics
- Hsu, J., Mao, J., Tenenbaum, J., & Wu, J. (2024). What’s left? concept grounding with logic-enhanced foundation models. Advances in Neural Information Processing Systems, 36. [link](https://arxiv.org/pdf/2404.19696)
- Liu, W., Chen, G., Hsu, J., Mao, J., & Wu, J. (2024). Learning Planning Abstractions from Language. arXiv preprint arXiv:2405.03864. [link](https://openreview.net/pdf?id=3UWuFoksGb)
- Feng, C., Hsu, J., Liu, W., & Wu, J. (2024). Naturally supervised 3d visual grounding with language-regularized concept learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 13269-13278). [link](https://arxiv.org/pdf/2404.19696)
- Zhen, H., Qiu, X., Chen, P., Yang, J., Yan, X., Du, Y., ... & Gan, C. (2024). 3d-vla: A 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631. [link](https://arxiv.org/pdf/2403.09631)
#### Energy-Based Models
- Du, Y., Mao, J., & Tenenbaum, J. B. Learning Iterative Reasoning through Energy Diffusion. In Forty-first International Conference on Machine Learning. [link](https://arxiv.org/pdf/2406.11179v1)
- Du, Y., Mao, J., & Tenenbaum, J. B. Learning Iterative Reasoning through Energy Diffusion. In Forty-first International Conference on Machine Learning. [link](https://proceedings.mlr.press/v162/du22d/du22d.pdf) \
  They propose Energy-Based Models as a methodology for reasoning, with a learning an energy landscape.

#### Read World Simulators
- Yang, M., Du, Y., Ghasemipour, K., Tompson, J., Schuurmans, D., & Abbeel, P. (2023). Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114. [link](https://arxiv.org/pdf/2310.06114)

#### Compositional Generative Modeling
- Du, Y., & Kaelbling, L. (2024). Compositional Generative Modeling: A Single Model is Not All You Need. arXiv preprint arXiv:2402.01103. [link](https://arxiv.org/pdf/2402.01103) 
#### Universal Policies
- Du, Y., Yang, S., Dai, B., Dai, H., Nachum, O., Tenenbaum, J., ... & Abbeel, P. (2024). Learning universal policies via text-guided video generation. Advances in Neural Information Processing Systems, 36. [link](https://arxiv.org/pdf/2302.00111)

### Lectures
- [Physics-informed machine learning –
Hype or new trend in computational engineering?](https://homepage.tudelft.nl/y95n9/files/presentations/icsm2023.pdf)




### Media & Web Resouces
Alexander Leschik. [Forecasting with Physics Informed Machine Learning (PIML)](https://medium.com/@royaltokens/forecasting-with-physics-informed-machine-learning-piml-670a6e168293)
### Key Advances

1. **Hybrid Models Combining Physics and Data-Driven Approaches**
   - **Physics-Guided Neural Networks (PGNNs)**: PGNNs incorporate physical laws directly into the training process, reducing the need for large datasets and enhancing model interpretability.
   - **Differentiable Physics**: This approach integrates differentiable physics solvers with neural networks, allowing models to learn physical dynamics more effectively.

2. **Physics-Informed Neural Networks (PINNs)**
   - **Improved Training Algorithms**: Advances in optimization techniques, such as adaptive learning rates and physics-based regularization methods, have significantly improved the training efficiency of PINNs.
   - **Multi-Scale and Multi-Physics Problems**: Recent work has extended PINNs to handle complex multi-scale and multi-physics problems, making them applicable to a wider range of scientific and engineering challenges.

3. **Graph Neural Networks (GNNs) for Physical Systems**
   - **Graph-based Representations**: GNNs have been utilized to model the interactions within physical systems, such as molecular dynamics and material science, by representing systems as graphs where nodes correspond to particles or atoms and edges represent interactions.
   - **Scalable and Efficient Computations**: Improvements in GNN architectures and training methods have enabled the scalable and efficient simulation of large physical systems.

4. **Integration with High-Performance Computing (HPC)**
   - **Accelerated Simulations**: The integration of LLMs with HPC resources has accelerated simulations, allowing for real-time data analysis and faster iteration cycles in research and development.
   - **Parallel Training and Inference**: Advances in parallel computing have facilitated the training and deployment of large-scale physics-informed models, enabling their use in complex and computationally intensive applications.

5. **Applications in Scientific Research**
   - **Climate Modeling**: Physics-informed LLMs have been applied to improve climate models, enhancing predictions of weather patterns, climate change impacts, and extreme weather events.
   - **Biomedical Engineering**: These models have been used to simulate biological systems, aiding in drug discovery, personalized medicine, and the understanding of complex biological processes.
   - **Engineering and Material Science**: Physics-informed LLMs have been utilized in the design and analysis of new materials, optimizing their properties for various applications.

### Challenges and Future Directions

1. **Scalability and Generalization**
   - **Scalability**: Ensuring that physics-informed LLMs can scale to handle large and complex systems remains a significant challenge.
   - **Generalization**: Developing models that generalize well across different physical systems and conditions is crucial for broader applicability.

2. **Interdisciplinary Collaboration**
   - **Collaboration**: Continued progress will require close collaboration between domain experts in physics, machine learning, and high-performance computing.

3. **Model Interpretability**
   - **Interpretability**: Enhancing the interpretability of physics-informed LLMs is essential for gaining insights into physical phenomena and for their acceptance in scientific and engineering communities.

### Conclusion

The field of physics-informed large language models is rapidly advancing, driven by innovations in model architectures, training techniques, and applications across various scientific domains. These models hold great promise for accelerating scientific discovery and engineering innovation by providing powerful tools that blend the strengths of machine learning with the rigor of physical laws.

As research continues, addressing challenges related to scalability, generalization, and interpretability will be key to unlocking the full potential of physics-informed LLMs. The future of this interdisciplinary field looks promising, with exciting opportunities for breakthroughs in both fundamental science and practical applications.


